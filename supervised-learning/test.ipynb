{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from evaluation import process_data_ensemble, run_ensemble\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test 0\n",
      "Running Test 1\n",
      "Running Test 2\n",
      "Running Test 3\n",
      "Running Test 4\n",
      "Running Test 5\n",
      "Running Test 6\n",
      "Running Test 7\n",
      "Running Test 8\n",
      "Running Test 9\n",
      "Running Test 10\n",
      "Running Test 11\n",
      "Running Test 12\n",
      "Running Test 13\n",
      "Running Test 14\n",
      "Running Test 15\n",
      "Running Test 16\n",
      "Running Test 17\n",
      "Running Test 18\n",
      "Running Test 19\n",
      "Running Test 20\n",
      "Running Test 21\n",
      "Running Test 22\n",
      "Running Test 23\n",
      "Running Test 24\n",
      "Test Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Training Data\n",
    "model = load_model('best_model.h5')\n",
    "# x_raw, y_raw = pickle.load(open('raw_data.pkl', 'rb'))\n",
    "test_traj = pickle.load(open('test_trajectories.pkl', 'rb'))\n",
    "test_labels = pickle.load(open('test_labels.pkl', 'rb'))\n",
    "\n",
    "# Save Predictions\n",
    "predictions = []\n",
    "\n",
    "i = 0\n",
    "for test in test_traj:\n",
    "    if len(test) > 1:\n",
    "        print(f'Running Test {i}')\n",
    "        i +=1\n",
    "        data = process_data_ensemble(test)\n",
    "        [p, nop, gen] = data\n",
    "        if p.shape == (2000, 3):\n",
    "            result = run_ensemble(data, model)\n",
    "            predictions.append(result)\n",
    "        else:\n",
    "            print('Not enough Values to unpack')\n",
    "            print(p.shape)\n",
    "            predictions.append(-1)\n",
    "    else:\n",
    "        print('Too Short')\n",
    "    \n",
    "correct = [1 if predictions[i] == test_labels[i] else 0 for i in range(len(predictions))]\n",
    "print(f'Test Accuracy: {sum(correct)/len(correct)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ft-gpu': conda)",
   "language": "python",
   "name": "python37664bitftgpuconda061a0cd85af341578c1781e42b881a23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
